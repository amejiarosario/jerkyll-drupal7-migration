<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: big-o | Adrian Mejia's [code]Blog]]></title>
  <link href="http://adrianmejia.com/blog/categories/big-o/atom.xml" rel="self"/>
  <link href="http://adrianmejia.com/"/>
  <updated>2013-02-02T14:17:52-05:00</updated>
  <id>http://adrianmejia.com/</id>
  <author>
    <name><![CDATA[Adrian Mejia]]></name>
    <email><![CDATA[me@adrianmejia.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Algorithms for Dummies (Part 1)]]></title>
    <link href="http://adrianmejia.com/blog/2013/01/28/algorithms-for-dummies-part-1/"/>
    <updated>2013-01-28T20:03:00-05:00</updated>
    <id>http://adrianmejia.com/blog/2013/01/28/algorithms-for-dummies-part-1</id>
    <content type="html"><![CDATA[<p>Right now, you have two options: you spend the rest of your life trying to improve your programming skills by trial-and-error -or- you spent some time studying algorithms and be awesome instead. I guess you are here for the second purpose: learn to design efficient algorithms!</p>

<p>First of all, we need to know how bad is our actual code to be able to improve it</p>

<p>"If you can not measure it, you can not improve it." ~ Lord Kelvin</p>

<p><strong>Note</strong>: this tutorial will be as practical as it could be. No pseudocode but real working code. No too much theory, just the enough amount that we need right now.
Why? because there are a lot of fat books full of theory, maths formulas and pseudocode. This is a lean attempt to get you started writing efficient code as soon as posible.</p>

<h1>Measuring your code</h1>

<p>You can give the same problem to 10 developers and most likely they will write 10 diffent programs. Which one is more efficient? Which one runs faster on most of the cases? To answer these questions we are going to learn to do run-time analysis of algorithms using the big-oh.</p>

<p>Big O notation allows us to classify algorithms by their procesing time in function of the program input. The bigger is the data a program has to process, the longer it will take to process it. However, some programs behaves differently when the input grows. Some of them will take will increase their processing time proportionally with the increase of input elements length (linear), other will take much more longer (cuadratic, cubic, exponential) time with the same increase of the input elements.</p>

<p>The following graph show us the orders of growth of the mentioned functions:</p>

<p><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%203.22.12%20PM.png" style="width: 300px; height: 306px; " /></p>

<p>If we represent the number of input elements as 'n' we have the following table.</p>

<table border=1>
<tr><td>Growth Rate</td><td>Name</td></tr>
<tr><td>1</td><td>Constant</td></tr>
<tr><td>log(n)</td><td>Logarithmic</td></tr>
<tr><td>n</td><td>Linear</td></tr>
<tr><td>n*log(n)</td><td>Linearithmic</td></tr>
<tr><td>n^2</td><td>Quadratic</td></tr>
<tr><td>n^3</td><td>Cubic</td></tr>
<tr><td>2^n</td><td>Exponential</td></tr>
</table>


<p></p>

<p>This is kinda abstract let's see what it means in code:</p>

<table border=1>
<tr><td>Growth Rate</td><td>Name</td><td>Code e.g.</td><td>description</td></tr>
<tr><td>1</td><td>Constant</td><td>a=1+3;</td><td>statement (one line of code)</td></tr>
<tr><td>log(n)</td><td>Logarithmic</td><td>while(n>1){n=n/2;}</td><td>Divide in half (binary search)</td></tr>
<tr><td>n</td><td>Linear</td><td>for(c=0;c&lt;n;c++){a=1+n;}</td><td>Loop</td></tr>
<!--<tr><td>n*log(n)</td><td>Linearithmic</td><td></td><td></td></tr>-->
<tr><td>n^2</td><td>Quadratic</td><td>for(c=0;c&lt;n;c++){for(i=0;i&lt;n;i++){…}}</td><td>Double loop</td></tr>
<tr><td>n^3</td><td>Cubic</td><td>for(c=0;c&lt;n;c++){for(i=0;i&lt;n;i++){for(x=0;x&lt;n;x++){…}}}</td><td>Triple loop</td></tr>
<!--<tr><td>2^n</td><td>Exponential</td></tr>-->
</table>


<p></p>

<h2>Cases</h2>

<p>You might have notice that when you run a program it takes different amount of time to compute the result, even with the same number of inputs. Why? because the order of the input matters! We have 3 basic cases: (1) worse-case scenario, (2) average-case scenario and (3) best-case scenario. Well there is a fouth which is in the dimension of space instead of time: (4) worse-case space scenario (also called worse-case space complexity).</p>

<ol>
<li>Worse case performance (Big Theta):</li>
<li>Best case performance (Big Omega):</li>
<li>Average case performance (Big O):</li>
</ol>


<p>kjsd</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Algorithms from Scratch / Algorithms for Dummies]]></title>
    <link href="http://adrianmejia.com/blog/2011/12/22/learning-algorithms-from-scratch-algorithms-for-dummies/"/>
    <updated>2011-12-22T00:00:00-05:00</updated>
    <id>http://adrianmejia.com/blog/2011/12/22/learning-algorithms-from-scratch-algorithms-for-dummies</id>
    <content type="html"><![CDATA[<p>When you are programming you face challenges all the way. Getting the problems solved is just the tip of the iceberg, getting it done efficiently is the rest.</p>


<p class="p1"><b>Why should you care for efficiency?</b></p>


<p class="p1">Solutions to the same problem might take years with certain algorithm, and just minutes using efficient algorithms. For instance, if you have applications that are used for thousands of people over internet, every fraction of second counts. Therefore, efficient algorithms is a must.</p>


<p class="p1"><b>How I do my algorithms more efficient?</b></p>


<p class="p1">To improve something you first need to know the actual state. In this case you need to measure the actual effectiveness of your algorithm in other to improve it. It&#39;s very common to use running time analysis to measure the speed of algorithms independently from the hardware used (old pc, supercomputer it doesn&#39;t matter).&nbsp;</p>


<p class="p1"><b>Run-time analysis</b></p>


<p class="p1">A common way to analyze the algorithms is using the big-O notation. The good thing about this notation is that is independent from the computer used to run the algorithm. You know that if you use a very slow computer (e.g. pentium I) v.s. a supercomputer use in NASA, the latter will run the program much faster. Big-O notation abstract the hardware and just focus in the algorithm per se. The only variable in the big-O notation gives the relative time needed to process an algorithm in function of the input n. Let&#39;s clarify this with an example.</p>


<p class="p1"><strong>Ex.1</strong> - You want to sort an array A of n integers.&nbsp;</p>


<p class="p1">Depending in the algorithm used to do that you may have:</p>


<ul>
    <li class="p1">
        <b>selection</b> sort has a running time of O(n^2);</li>
    <li class="p1">
        <b>merge sort</b> --&gt; O(n log n)</li>
</ul>


<p class="p1">Right now, it doesn&#39;t matter if are not familiar with these algorithms (we will cover this the next lessons), the point here is that we have n integer and big-O notations give us a mathematical expression that is in function of the input n. If you&nbsp;<a href="http://fooplot.com/index.php?&amp;type0=0&amp;type1=0&amp;type2=0&amp;type3=0&amp;type4=0&amp;y0=x%5E2&amp;y1=x*log%28x%29&amp;y2=&amp;y3=&amp;y4=&amp;r0=&amp;r1=&amp;r2=&amp;r3=&amp;r4=&amp;px0=&amp;px1=&amp;px2=&amp;px3=&amp;px4=&amp;py0=&amp;py1=&amp;py2=&amp;py3=&amp;py4=&amp;smin0=0&amp;smin1=0&amp;smin2=0&amp;smin3=0&amp;smin4=0&amp;smax0=2pi&amp;smax1=2pi&amp;smax2=2pi&amp;smax3=2pi&amp;smax4=2pi&amp;thetamin0=0&amp;thetamin1=0&amp;thetamin2=0&amp;thetamin3=0&amp;thetamin4=0&amp;thetamax0=2pi&amp;thetamax1=2pi&amp;thetamax2=2pi&amp;thetamax3=2pi&amp;thetamax4=2pi&amp;ipw=0&amp;ixmin=-5&amp;ixmax=5&amp;iymin=-3&amp;iymax=3&amp;igx=1&amp;igy=1&amp;igl=1&amp;igs=0&amp;iax=1&amp;ila=1&amp;xmin=-5&amp;xmax=5&amp;ymin=-3&amp;ymax=3"><span class="s1">plot in a graph n^2 and n log n</span></a>. You&#39;ll see that n^2 grows much faster than n log(n). That means that the algorithm n^2 will take longer than n*log(n) to process as the size of the array n increases.</p>


<p class="p1"><b>Common order of Growth</b></p>


<p class="p1">To give you an idea of the common order of growth of runtime expressions. Take a look at the following graph and table. The slower the function growth the better is the algorithm. In order from better performance to worst is:</p>


<p class="p1">1 -- log n -- n -- n log n -- n^2 -- n^3 -- 2^n -- n! ...</p>


<p class="p2"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%203.22.12%20PM.png" style="width: 300px; height: 306px; " /></p>


<p class="p2">&nbsp;</p>


<p class="p2"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%203.23.45%20PM.png" style="width: 300px; height: 257px; " /></p>


<p class="p2">&nbsp;</p>


<p class="p1">&nbsp;</p>


<p class="p1"><b>Approximate growth rate from code.</b></p>


<p class="p1">There are a whole theory and math behind the Big-O notation and other notations related. At this time, just take a look of the typical code and its growth order.</p>


<p class="p1"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%204.51.48%20PM.png" style="width: 600px; height: 427px; " /></p>


<p>&nbsp;</p>


<p><strong>Cases (the good, the bad, and the ugly)</strong></p>


<p>Remember that n is the number of elements in the input. All this runtime growth rate are in function of the input elements. There is another important thing to consider about the input elements: the order! The order of the input elements matters, and that&#39;s why algorithms are analyzed in 3 different cases:</p>


<ol>
    <li>
        Worst-case performance: the input is distributed as worst as it could be for an algorithm. &nbsp;&nbsp;</li>
    <li>
        Average-case scenario: approximation of the most common arrange of inputs.</li>
    <li>
        Best-case scenario: most favorable distribution of the inputs.</li>
    <li>
        One more: Space. this is how much space the algorithm cosume to execute.&nbsp;</li>
</ol>


<p class="p2">If you want more depth in these topic read here:&nbsp;</p>


<ul>
    <li class="p2">
        <span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">Analysis (</span><a href="http://gcu.googlecode.com/files/02Analysis.pdf" style="color: rgb(85, 26, 139); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">pdf</a><span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">) (</span><a href="http://gcu.googlecode.com/files/02Analysis.key.zip" style="color: rgb(85, 26, 139); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">keynote</a><span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">)</span></li>
    <li class="p2">
        <span style="background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-size: small; line-height: 16px; ">Algorithm @&nbsp;</span>ocw.mit.edu: lectures <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-1-administrivia-introduction-analysis-of-algorithms-insertion-sort-mergesort">1 </a>and <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-2-asymptotic-notation-recurrences-substitution-master-method">2</a></li>
    <li class="p2">
        http://algs4.cs.princeton.edu/home/</li>
</ul>


<p class="p2">&nbsp;</p>

]]></content>
  </entry>
  
</feed>
